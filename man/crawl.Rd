% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/harvest-docs.R
\name{crawl}
\alias{crawl}
\title{Scrape and process all hyperlinks within a given URL}
\usage{
crawl(
  url,
  index_create = TRUE,
  aggressive = FALSE,
  overwrite = FALSE,
  pkg_version = NULL,
  pkg_name = NULL,
  service = "openai"
)
}
\arguments{
\item{url}{A character string with the URL to be scraped.}

\item{index_create}{A logical value indicating whether to create an index.
Default is TRUE.}

\item{aggressive}{A logical value indicating whether to use aggressive link
crawling. Default is FALSE.}

\item{overwrite}{A logical value indicating whether to overwrite scraped
pages and index if they already exist. Default is FALSE.
\code{parallel::detectCores() - 1}}

\item{pkg_version}{Package version number}

\item{pkg_name}{Package name}

\item{service}{The service to use for scraping. Default is "openai". Options
are "openai" and "local".}
}
\value{
NULL. The resulting tibble is saved into a parquet file.
}
\description{
This function scrapes all hyperlinks within a given URL and processes the
data into a tibble format. It saves the resulting tibble into a parquet file.
}
