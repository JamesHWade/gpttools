% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/harvest-docs.R
\name{crawl}
\alias{crawl}
\title{Scrape and process all hyperlinks within a given URL}
\usage{
crawl(
  url,
  index_create = TRUE,
  aggressive = FALSE,
  overwrite = FALSE,
  num_cores = parallel::detectCores() - 1,
  pkg_version = NULL,
  use_azure_openai = FALSE
)
}
\arguments{
\item{url}{A character string with the URL to be scraped.}

\item{index_create}{A logical value indicating whether to create an index.
Default is TRUE.}

\item{aggressive}{A logical value indicating whether to use aggressive link
crawling. Default is FALSE.}

\item{overwrite}{A logical value indicating whether to overwrite scraped
pages and index if they already exist. Default is FALSE.}

\item{num_cores}{Number of cores to use. Defaults to
\code{parallel::detectCores() - 1}}

\item{pkg_version}{Package version number}

\item{use_azure_openai}{Whether to use Azure OpenAI for index creation}
}
\value{
NULL. The resulting tibble is saved into a parquet file.
}
\description{
This function scrapes all hyperlinks within a given URL and processes the
data into a tibble format. It saves the resulting tibble into a parquet file.
}
